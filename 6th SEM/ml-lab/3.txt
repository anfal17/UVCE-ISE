# 3rd ID3 , dataset below

import numpy as np
import pandas as pd

dataset = pd.read_csv('3.csv', names=['outlook', 'temperature', 'humidity', 'wind', 'class'])

attributes = ('Outlook', 'Temperature', 'Humidity', 'Wind', 'PlayTennis')

# Function to calculate entropy
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy = np.sum([(-counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])
    return entropy

# Function to calculate information gain
def InfoGain(data, split_attribute_name, target_name="class"):
    total_entropy = entropy(data[target_name])  # Calculate the entropy of the total dataset
    vals, counts = np.unique(data[split_attribute_name], return_counts=True)

    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])

    information_gain = total_entropy - weighted_entropy
    return information_gain

def ID3(data, originaldata, features, target_attribute_name="class", parent_node_class=None):
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]  # If all target values have the same value, return this value
    elif len(data) == 0:
        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]  # If dataset is empty, return the mode of the original data
    elif len(features) == 0:
        return parent_node_class  # If no more features, return the parent class

    parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]

    item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]
    best_feature_index = np.argmax(item_values)
    best_feature = features[best_feature_index]

    tree = {best_feature: {}}

    features = [i for i in features if i != best_feature]

    for value in np.unique(data[best_feature]):
        sub_data = data.where(data[best_feature] == value).dropna()  # Split the dataset based on the best feature value
        subtree = ID3(sub_data, originaldata, features, target_attribute_name, parent_node_class)
        tree[best_feature][value] = subtree  # Add the subtree to the tree

    return tree

def predict(query, tree, default=1):
    for key in list(query.keys()):
        if key in list(tree.keys()):
            try:
                result = tree[key][query[key]]
            except:
                return default
            result = tree[key][query[key]]
            if isinstance(result, dict):
                return predict(query, result)
            else:
                return result

def train_test_split(dataset):
    training_data = dataset.iloc[:14].reset_index(drop=True)  
    return training_data

def test(data, tree):
    queries = data.iloc[:, :-1].to_dict(orient="records")  # Convert data (except target) to a dictionary
    predicted = pd.DataFrame(columns=["predicted"])  # DataFrame to store predictions

    for i in range(len(data)):
        predicted.loc[i, "predicted"] = predict(queries[i], tree, 1.0)

    accuracy = (np.sum(predicted["predicted"] == data["class"]) / len(data)) * 100
    print('The prediction accuracy is: ', accuracy, '%')

XX = train_test_split(dataset)
training_data = XX
tree = ID3(training_data, training_data, training_data.columns[:-1])
print('Display Tree:', tree)
print('len =', len(training_data))
test(training_data, tree)


#--------------- dataset 3.csv ----------------

0,0,0,0,0
0,0,0,1,0
1,0,0,0,1
2,1,0,0,1
2,2,1,0,1
2,2,1,1,0
1,2,1,1,1
0,1,0,0,0
0,2,1,0,1
2,1,1,0,1
0,1,1,1,1
1,1,0,1,1
1,0,1,0,1
2,1,0,1,0
0,1,1,1,1
1,1,1,1,1

# ------------------ output ----------------
"""
Display Tree {'outlook': {0: {'humidity': {0.0: 0.0, 1.0: 1.0}}, 1: 1.0, 2: {'wind': {0.0: 1.0, 1.0: 0.0}}}}
len= 16
The prediction accuracy is: 100.0%
The predicted class for the new sample is: 0
"""

